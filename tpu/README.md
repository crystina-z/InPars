# Finetuning and evaluation of monoT5 on TPUs using synthetic data generated by InPars

## Getting the data

Let's first download all the data which are:

* Synthetic queries
* BM25 runs from
* Qrels
* Queries
* Corpus
  
Be sure to have at least 350GB available on disk and follow this [guide](https://github.com/castorini/pygaggle/blob/master/docs/experiments-monot5-tpu.md#setup-environment-on-vm) to install T5 dependencies.

```
pip install -U pyserini
pip install wget ir-measures
cd tpu/
nohup python -u download_data.py &
```

# Finetuning on synthetic data
Use a TPU v3-8 to train on each dataset:
```
nohup python -u train_inpars.py \
    --gcp_path path_to_gcp_bucket \
    --tpu_proj project_name \
    --tpu_name tpu_name &
```

# Retrieval
Use a TPU v3-8 to retrieve on each dataset:
```
nohup python run_t5_3B_inpars.py \
    --gcp_path path_to_gcp_bucket \
    --tpu_proj project_name \
    --tpu_name tpu_name &
```

# Download and evaluate

Download from the GPC bucket the retrieval's scores of each dataset and compute the IR metrics:
```
nohup python get_t5_3B_inpars.py \
    --gcp_path path_to_gcp_bucket \
    --tpu_proj project_name \
    --tpu_name tpu_name &
```

# Finetuned models
The models finetuned on BEIR are available on Huggingface:
https://huggingface.co/zeta-alpha-ai
